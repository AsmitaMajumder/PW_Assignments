{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "Ans:A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm primarily used for classification and, in some cases, regression tasks. It is particularly effective for problems where the data is high-dimensional or not linearly separable\n",
        "\n",
        "It works:\n",
        "\n",
        "1.\tIdentify the Optimal Hyperplane:\n",
        "o\tSVM searches for the hyperplane that maximizes the margin between classes.\n",
        "o\tThis ensures better separation and generalization to unseen data.\n",
        "2.\tUse of Support Vectors:\n",
        "o\tOnly the support vectors influence the position of the hyperplane.\n",
        "o\tThese points are used to calculate the margin and optimize the decision boundary.\n",
        "3.\tHandling Non-Linearly Separable Data:\n",
        "o\tSVM uses kernel functions (e.g., polynomial, RBF) to transform data into higher dimensions where a linear separator is possible.\n",
        "4.\tRegularization (C Parameter):\n",
        "o\tControls the trade-off between maximizing the margin and minimizing classification error.\n",
        "o\tA higher C penalizes misclassifications more strictly.\n",
        "5.\tLoss Function (Hinge Loss):\n",
        "o\tPenalizes incorrect classifications and margin violations.\n",
        "o\tCombined with regularization to form the optimization objective\n"
      ],
      "metadata": {
        "id": "NVXL_AjsNTrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "Ans:\n",
        "Hard Margin\n",
        "Maximize margin.\n",
        "Sensitive, requires perfectly linearly separable data\n",
        "Not applicable, no regularization parameter\n",
        "Simple, computationally efficient\n",
        "\n",
        "Soft Margin\n",
        "Maximize margin, minimize margin violations.\n",
        "Robust, handles noisy data with margin violations.\n",
        "Controlled by regularization parameter C.\n",
        "May require more computational resources\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7lIb1bjiN9Dm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "Ans:\n",
        "The Kernel Trick is a mathematical technique used in Support Vector Machines (SVMs) to handle non-linearly separable data by implicitly mapping it into a higher-dimensional space where it becomes linearly separable — without explicitly computing that transformation.\n",
        "\n",
        "Use Case Example:\n",
        " classifying handwritten digits (like MNIST dataset). The digit images are not linearly separable, but by using an RBF kernel, the SVM can find a nonlinear decision boundary that separates digits effectively in a high-dimensional space."
      ],
      "metadata": {
        "id": "zYCabIMKmfwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "Ans:\n",
        "\n",
        "A Naïve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem.\n",
        "It is commonly used for classification tasks, such as spam detection, sentiment analysis, and text categorization.\n",
        "\n",
        "It predicts the class of a given data point based on probabilities calculated from prior knowledge of the data.\n"
      ],
      "metadata": {
        "id": "VLXs2_Vvm9Fx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.When would you use each one?\n",
        "\n",
        "Ans:\n",
        "Gaussian Naïve Bayes\n",
        "Assumes that features follow a normal (Gaussian) distribution within each class.\n",
        "It models the likelihood of continuous features using the probability density function (PDF) of the normal distribution.\n",
        "\n",
        "Multinomial Naïve Bayes: features represent discrete counts (like the number of times a word appears).The likelihood is modeled using a multinomial distribution\n",
        "\n",
        "Bernoulli Naïve Bayes\n",
        "features are binary (0 or 1) — indicating presence or absence of a feature.\n",
        "The model calculates the likelihood of each binary feature using\n",
        "\n",
        "When we should use:\n",
        "Gaussian Naïve Bayes → for continuous data,\n",
        "Multinomial Naïve Bayes → for count-based data,\n",
        "Bernoulli Naïve Bayes → for binary presence/absence data."
      ],
      "metadata": {
        "id": "JtUhYr_nnUih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 6:\n",
        "● You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        " Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors."
      ],
      "metadata": {
        "id": "tTzPKuuqn6zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data      # Features\n",
        "y = iris.target    # Target labels\n",
        "\n",
        "# 2. Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Create and train the SVM classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 5. Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
        "\n",
        "# 6. Print the support vectors\n",
        "print(\"\\nSupport Vectors:\\n\", svm_model.support_vectors_)\n",
        "print(\"\\nNumber of Support Vectors for Each Class:\", svm_model.n_support_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wPE3Q3PoWYe",
        "outputId": "52d06ddf-a968-40eb-bbd5-7ce780a7f5bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.0 %\n",
            "\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "\n",
            "Number of Support Vectors for Each Class: [ 3 11 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "6fLes-B8opPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data      # Features\n",
        "y = breast_cancer.target    # Target labels\n",
        "\n",
        "# 2. Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Create and train the Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Print classification report (precision, recall, F1-score)\n",
        "print(\"Classification Report for Gaussian Naïve Bayes:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5bk1sRhouXg",
        "outputId": "c5d30b0b-499f-48a7-a991-19461f72a1a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Gaussian Naïve Bayes:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n"
      ],
      "metadata": {
        "id": "DIdl0Pghozdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data      # Features\n",
        "y = wine.target    # Labels\n",
        "\n",
        "# 2. Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Define the SVM model\n",
        "svm_model = SVC()\n",
        "\n",
        "# 4. Define the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']  # RBF kernel works well for most datasets\n",
        "}\n",
        "\n",
        "# 5. Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(svm_model, param_grid, refit=True, cv=5, verbose=1)\n",
        "\n",
        "# 6. Fit the model using GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 7. Make predictions on the test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# 8. Print the best hyperparameters and accuracy\n",
        "print(\"\\nBest Hyperparameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nTest Set Accuracy:\", round(accuracy * 100, 2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTmwvZ8Xo6y3",
        "outputId": "0bc3110a-5115-41c3-c6c6-ca8e5f0ba5ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "\n",
            "Best Hyperparameters found by GridSearchCV:\n",
            "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            "Test Set Accuracy: 83.33 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 9: Write a Python program to:\n",
        "Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        " Print the model's ROC-AUC score for its predictions.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "09LcqILro_iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# 1. Load the 20 Newsgroups dataset (using a few categories for simplicity)\n",
        "categories = ['sci.space', 'rec.sport.hockey', 'comp.graphics', 'talk.politics.misc']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X = newsgroups.data   # Text data\n",
        "y = newsgroups.target # Target labels\n",
        "\n",
        "# 2. Split into training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Convert text to numerical feature vectors using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# 4. Train a Multinomial Naïve Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 5. Predict probabilities for ROC-AUC calculation\n",
        "y_prob = nb_model.predict_proba(X_test_tfidf)\n",
        "\n",
        "# 6. Compute ROC-AUC score (for multiclass classification)\n",
        "# We binarize the labels for multi-class ROC-AUC\n",
        "y_test_binarized = label_binarize(y_test, classes=range(len(categories)))\n",
        "roc_auc = roc_auc_score(y_test_binarized, y_prob, average='macro', multi_class='ovr')\n",
        "\n",
        "# 7. Print the ROC-AUC score\n",
        "print(\"ROC-AUC Score for Multinomial Naïve Bayes Model:\", round(roc_auc, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UycbZNSjpKU0",
        "outputId": "e9725db4-822e-4df9-c304-7af751e521b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score for Multinomial Naïve Bayes Model: 0.9826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        " Text with diverse vocabulary\n",
        " Potential class imbalance (far more legitimate emails than spam)\n",
        " Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        " Preprocess the data (e.g. text vectorization, handling missing data)\n",
        " Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        " Address class imbalance\n",
        " Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "Ans:\n",
        "\n",
        "Approach to Spam Email Classification\n",
        "1. Data Preprocessing:\n",
        "Handle missing text → remove or replace with \"unknown\".\n",
        "Clean text: lowercase, remove punctuation, stopwords, URLs.\n",
        "Use TF-IDF vectorization (with n-grams) to convert text into numerical form.\n",
        "Apply stemming/lemmatization to normalize words.\n",
        "________________________________________\n",
        "2. Model Choice:\n",
        "Use Multinomial Naïve Bayes → fast, works well for text and word frequencies.\n",
        "SVM can be tested later for higher accuracy but is slower.\n",
        "________________________________________\n",
        "3. Handling Class Imbalance:\n",
        "Using SMOTE (oversampling) or class_weight='balanced'.\n",
        "Adjust decision threshold to improve spam recall.\n",
        "________________________________________\n",
        "4. Evaluation Metrics:\n",
        "Use Precision, Recall, F1-score, and ROC-AUC (not just accuracy).\n",
        "Aim for high recall (catch more spam) while maintaining good precision.\n",
        "________________________________________\n",
        "5. Business Impact:\n",
        "Filters spam automatically → saves time and boosts productivity.\n",
        "Reduces phishing risks and builds user trust.\n",
        "Keeps communication efficient and secure\n",
        "\n"
      ],
      "metadata": {
        "id": "1kZMj04zpXB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KoxHvZg1ONhu"
      }
    }
  ]
}